{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c24a8994",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3522c6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_excel(\"data/diy/sample_data_1.xlsx\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183baebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contoh: styling kolom agar wrap\n",
    "def display_fullscreen_wrap(df):\n",
    "    return df.style.set_properties(\n",
    "        **{\n",
    "            \"white-space\": \"pre-wrap\",  # wrap isi cell\n",
    "            \"word-break\": \"break-word\",  # pecah di mana saja kalau kepanjangan\n",
    "            \"width\": \"900px\",  # bisa diganti sesuai kebutuhan\n",
    "            \"max-width\": \"700px\",  # atur lebar kolom maksimal\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab7b6b8",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b50e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600cbcd2",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce28678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "info = data.info()\n",
    "print(\"Summary :\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c132796",
   "metadata": {},
   "source": [
    "### Find Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313c1ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# menghitung dan menampilkan missing values\n",
    "print(\"Jumlah missing values disetiap kolom:\\n\", data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6c8811",
   "metadata": {},
   "source": [
    "### Delete unnecessary columns and does not provide any information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c7391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[[\"full_text\"]]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3325fc7",
   "metadata": {},
   "source": [
    "### Check Duplicate Data that contain in the content column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6706c032",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7b1888",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb6c124",
   "metadata": {},
   "source": [
    "### Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fa2f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merubah jenis huruf menjadi huruf kecil\n",
    "data[\"full_text\"] = data[\"full_text\"].str.lower()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe18c8f",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0851b214",
   "metadata": {},
   "source": [
    "### Emoji To Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6e68f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from indoNLP.preprocessing import (\n",
    "    pipeline,\n",
    "    replace_word_elongation,\n",
    "    replace_slang,\n",
    "    remove_html,\n",
    "    remove_url,\n",
    "    emoji_to_words\n",
    ")\n",
    "# Apply emoji_to_words to the text column\n",
    "data[\"full_text\"] = data[\"full_text\"].apply(lambda x: emoji_to_words(str(x), lang=\"id\"))\n",
    "display_fullscreen_wrap(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589d6973",
   "metadata": {},
   "source": [
    "### Remove HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b30fe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"full_text\"] = data[\"full_text\"].apply(lambda x: remove_html(str(x)))\n",
    "# display_fullscreen_wrap(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839180c9",
   "metadata": {},
   "source": [
    "### Remove URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de13ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"full_text\"] = data[\"full_text\"].apply(lambda x: remove_url(str(x)))\n",
    "# display_fullscreen_wrap(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6824d2",
   "metadata": {},
   "source": [
    "### Replace Slang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88eb8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"full_text\"] = data[\"full_text\"].apply(lambda x: replace_slang(str(x)))\n",
    "# display_fullscreen_wrap(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da0b117",
   "metadata": {},
   "source": [
    "### Replace Word Elongation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f898e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"full_text\"] = data[\"full_text\"].apply(lambda x: replace_word_elongation(str(x)))\n",
    "display_fullscreen_wrap(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423bfe8e",
   "metadata": {},
   "source": [
    "### Replace User Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53adbb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to replace URLs and user mentions\n",
    "def replace_urls_and_mentions(text):\n",
    "    # Replace URLs with HTTPURL\n",
    "    text = re.sub(\n",
    "        r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n",
    "        \"HTTPURL\",\n",
    "        text,\n",
    "    )\n",
    "    text = re.sub(\n",
    "        r\"www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n",
    "        \"HTTPURL\",\n",
    "        text,\n",
    "    )\n",
    "\n",
    "    # Replace user mentions with @USER\n",
    "    text = re.sub(r\"@[A-Za-z0-9_]+\", \"@USER\", text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply URL and mention replacement\n",
    "data[\"full_text\"] = data[\"full_text\"].apply(lambda x: replace_urls_and_mentions(str(x)))\n",
    "display_fullscreen_wrap(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624c10ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Ollama status and fix connection issues\n",
    "\n",
    "import requests\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "def check_ollama_status():\n",
    "    \"\"\"Check if Ollama is running and what models are available\"\"\"\n",
    "    try:\n",
    "        # Check if Ollama is running\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json()\n",
    "            print(\"‚úÖ Ollama is running!\")\n",
    "            print(\"Available models:\")\n",
    "            for model in models.get(\"models\", []):\n",
    "                print(f\"  - {model['name']}\")\n",
    "            return True, models.get(\"models\", [])\n",
    "        else:\n",
    "            print(f\"‚ùå Ollama API returned status: {response.status_code}\")\n",
    "            return False, []\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"‚ùå Cannot connect to Ollama. Is it running?\")\n",
    "        print(\"To start Ollama, run: ollama serve\")\n",
    "        return False, []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error checking Ollama: {e}\")\n",
    "        return False, []\n",
    "\n",
    "\n",
    "def start_ollama():\n",
    "    \"\"\"Try to start Ollama if it's not running\"\"\"\n",
    "    try:\n",
    "        print(\"üîÑ Trying to start Ollama...\")\n",
    "        subprocess.Popen(\n",
    "            [\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
    "        )\n",
    "        print(\"‚úÖ Ollama started! Wait a few seconds for it to initialize...\")\n",
    "        import time\n",
    "\n",
    "        time.sleep(5)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to start Ollama: {e}\")\n",
    "        print(\"Please start Ollama manually by running 'ollama serve' in terminal\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def pull_model(model_name=\"llama3.1:8b\"):\n",
    "    \"\"\"Pull the model if it's not available\"\"\"\n",
    "    try:\n",
    "        print(f\"üîÑ Pulling model {model_name}...\")\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"pull\", model_name], capture_output=True, text=True, timeout=300\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ Model {model_name} pulled successfully!\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to pull model: {result.stderr}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error pulling model: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Check Ollama status\n",
    "print(\"Checking Ollama status...\")\n",
    "is_running, available_models = check_ollama_status()\n",
    "\n",
    "if not is_running:\n",
    "    print(\"\\nüîß Ollama is not running. Trying to start it...\")\n",
    "    if start_ollama():\n",
    "        # Recheck after starting\n",
    "        import time\n",
    "\n",
    "        time.sleep(3)\n",
    "        is_running, available_models = check_ollama_status()\n",
    "\n",
    "if is_running:\n",
    "    # Check if we have the required model\n",
    "    model_names = [model[\"name\"] for model in available_models]\n",
    "    required_models = [\"llama3.1:8b\", \"llama3:latest\", \"llama3.1\", \"llama3\"]\n",
    "\n",
    "    available_model = None\n",
    "    for model in required_models:\n",
    "        if any(model in available for available in model_names):\n",
    "            available_model = model\n",
    "            break\n",
    "\n",
    "    if not available_model:\n",
    "        print(f\"\\nüîÑ Required model not found. Available models: {model_names}\")\n",
    "        print(\"Pulling llama3.1:8b...\")\n",
    "        if pull_model(\"llama3.1:8b\"):\n",
    "            available_model = \"llama3.1:8b\"\n",
    "\n",
    "    if available_model:\n",
    "        print(f\"\\n‚úÖ Using model: {available_model}\")\n",
    "\n",
    "        # Update the function with correct model\n",
    "        def classify_hate_speech_with_ollama(text, model=available_model):\n",
    "            \"\"\"\n",
    "            Classify text for hate speech using Ollama Llama3\n",
    "            Returns: dict with 'is_hate_speech', 'target_type', 'confidence', 'reasoning'\n",
    "            \"\"\"\n",
    "\n",
    "            prompt = f\"\"\"\n",
    "Analisis teks berikut untuk menentukan apakah mengandung hate speech (ucapan kebencian) atau tidak.\n",
    "\n",
    "Hate Speech adalah tindakan komunikasi berupa provokasi, hasutan, atau hinaan kepada individu/kelompok berdasarkan ras, warna kulit, etnis, gender, cacat, orientasi seksual, kewarganegaraan, agama, status sosial ekonomi, pandangan politik, atau karakteristik identitas lainnya.\n",
    "\n",
    "Kriteria Hate Speech:\n",
    "1. Mengandung kata-kata kasar, cercaan, atau makian yang ditujukan pada kelompok tertentu\n",
    "2. Menyebarkan kebencian atau diskriminasi terhadap identitas seseorang\n",
    "3. Menghasut atau memprovokasi tindakan kekerasan terhadap individu/kelompok\n",
    "4. Merendahkan atau menghina berdasarkan karakteristik pribadi atau identitas\n",
    "5. Mengandung ancaman atau intimidasi terhadap kelompok tertentu\n",
    "\n",
    "Teks: \"{text}\"\n",
    "\n",
    "Berikan analisis dalam format JSON:\n",
    "{{\n",
    "    \"is_hate_speech\": true/false,\n",
    "    \"target_type\": \"individual/group/none\",\n",
    "    \"confidence\": \"high/medium/low\",\n",
    "    \"reasoning\": \"penjelasan singkat mengapa diklasifikasikan demikian\"\n",
    "}}\n",
    "\n",
    "Jawab hanya dengan JSON, tanpa penjelasan tambahan.\n",
    "\"\"\"\n",
    "\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    \"http://localhost:11434/api/generate\",\n",
    "                    json={\n",
    "                        \"model\": model,\n",
    "                        \"prompt\": prompt,\n",
    "                        \"stream\": False,\n",
    "                        \"options\": {\n",
    "                            \"temperature\": 0.1,\n",
    "                            \"top_p\": 0.9,\n",
    "                        },\n",
    "                    },\n",
    "                    timeout=60,\n",
    "                )\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    generated_text = result[\"response\"].strip()\n",
    "\n",
    "                    try:\n",
    "                        import re\n",
    "\n",
    "                        json_match = re.search(r\"\\{.*\\}\", generated_text, re.DOTALL)\n",
    "                        if json_match:\n",
    "                            json_str = json_match.group()\n",
    "                            classification = json.loads(json_str)\n",
    "                            return classification\n",
    "                        else:\n",
    "                            return {\n",
    "                                \"is_hate_speech\": False,\n",
    "                                \"target_type\": \"none\",\n",
    "                                \"confidence\": \"low\",\n",
    "                                \"reasoning\": \"Failed to parse response\",\n",
    "                            }\n",
    "                    except json.JSONDecodeError:\n",
    "                        return {\n",
    "                            \"is_hate_speech\": False,\n",
    "                            \"target_type\": \"none\",\n",
    "                            \"confidence\": \"low\",\n",
    "                            \"reasoning\": \"Invalid JSON response\",\n",
    "                        }\n",
    "                else:\n",
    "                    return {\n",
    "                        \"is_hate_speech\": False,\n",
    "                        \"target_type\": \"none\",\n",
    "                        \"confidence\": \"low\",\n",
    "                        \"reasoning\": f\"API error: {response.status_code}\",\n",
    "                    }\n",
    "\n",
    "            except Exception as e:\n",
    "                return {\n",
    "                    \"is_hate_speech\": False,\n",
    "                    \"target_type\": \"none\",\n",
    "                    \"confidence\": \"low\",\n",
    "                    \"reasoning\": f\"Error: {str(e)}\",\n",
    "                }\n",
    "\n",
    "        # Test the function with a sample text\n",
    "        sample_text = \"Selamat pagi, bagaimana kabar Anda hari ini?\"\n",
    "        test_result = classify_hate_speech_with_ollama(sample_text)\n",
    "        print(\"\\nüß™ Test classification result:\")\n",
    "        print(json.dumps(test_result, indent=2, ensure_ascii=False))\n",
    "\n",
    "    else:\n",
    "        print(\"‚ùå Could not setup any model\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Please start Ollama manually:\")\n",
    "    print(\"1. Open terminal\")\n",
    "    print(\"2. Run: ollama serve\")\n",
    "    print(\"3. In another terminal, run: ollama pull llama3.1:8b\")\n",
    "    print(\"4. Then run this cell again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb44a2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed Multi-Category Hate Speech Classification\n",
    "import json\n",
    "\n",
    "\n",
    "def classify_hate_speech_detailed(\n",
    "    text, model=\"qwen2.5:latest\"\n",
    "):  # ‚úÖ Gunakan model yang tersedia\n",
    "    \"\"\"\n",
    "    Classify text for hate speech with detailed categories\n",
    "    Returns: dict with multiple binary labels for different types of hate speech\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Kamu adalah ahli analisis hate speech bahasa Indonesia. Analisis teks berikut dengan SANGAT HATI-HATI.\n",
    "\n",
    "DEFINISI HATE SPEECH:\n",
    "- Ucapan yang mengandung KEBENCIAN, DISKRIMINASI, atau PENGHINAAN terhadap individu/kelompok\n",
    "- Berdasarkan ras, agama, gender, etnis, orientasi seksual, disabilitas, dll\n",
    "- Menghasut kekerasan atau menciptakan permusuhan\n",
    "\n",
    "YANG BUKAN HATE SPEECH:\n",
    "- Berita kecelakaan, bencana, kriminal\n",
    "- Kritik konstruktif\n",
    "- Keluhan layanan publik\n",
    "- Diskusi politik normal\n",
    "\n",
    "Teks: \"{text}\"\n",
    "\n",
    "Analisis dengan format JSON (nilai 1=ada, 0=tidak ada):\n",
    "{{\n",
    "    \"HS\": 0,\n",
    "    \"HS_INDIVIDU\": 0,\n",
    "    \"HS_KELOMPOK\": 0,\n",
    "    \"HS_RAS\": 0,\n",
    "    \"HS_GENDER\": 0,\n",
    "    \"HS_AGAMA\": 0,\n",
    "    \"HS_POLITIK\": 0,\n",
    "    \"HS_FISIK\": 0,\n",
    "    \"HS_SOSIAL\": 0,\n",
    "    \"confidence\": \"high\",\n",
    "    \"reasoning\": \"penjelasan detail\"\n",
    "}}\n",
    "\n",
    "HANYA jawab JSON, tidak ada teks lain.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False,\n",
    "                \"options\": {\n",
    "                    \"temperature\": 0.3,  # ‚úÖ Naikkan sedikit untuk fleksibilitas\n",
    "                    \"top_p\": 0.9,\n",
    "                },\n",
    "            },\n",
    "            timeout=60,\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            generated_text = result[\"response\"].strip()\n",
    "\n",
    "            try:\n",
    "                import re\n",
    "\n",
    "                json_match = re.search(r\"\\{.*\\}\", generated_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    json_str = json_match.group()\n",
    "                    classification = json.loads(json_str)\n",
    "\n",
    "                    # Ensure all required fields exist with default values\n",
    "                    default_result = {\n",
    "                        \"HS\": 0,\n",
    "                        \"HS_INDIVIDU\": 0,\n",
    "                        \"HS_KELOMPOK\": 0,\n",
    "                        \"HS_RAS\": 0,\n",
    "                        \"HS_GENDER\": 0,\n",
    "                        \"HS_AGAMA\": 0,\n",
    "                        \"HS_POLITIK\": 0,\n",
    "                        \"HS_FISIK\": 0,\n",
    "                        \"HS_SOSIAL\": 0,\n",
    "                        \"confidence\": \"low\",\n",
    "                        \"reasoning\": \"No classification\",\n",
    "                    }\n",
    "\n",
    "                    # Update with actual results\n",
    "                    default_result.update(classification)\n",
    "                    return default_result\n",
    "                else:\n",
    "                    return default_result\n",
    "            except json.JSONDecodeError:\n",
    "                return default_result\n",
    "        else:\n",
    "            return default_result\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"HS\": 0,\n",
    "            \"HS_INDIVIDU\": 0,\n",
    "            \"HS_KELOMPOK\": 0,\n",
    "            \"HS_RAS\": 0,\n",
    "            \"HS_GENDER\": 0,\n",
    "            \"HS_AGAMA\": 0,\n",
    "            \"HS_POLITIK\": 0,\n",
    "            \"HS_FISIK\": 0,\n",
    "            \"HS_SOSIAL\": 0,\n",
    "            \"confidence\": \"low\",\n",
    "            \"reasoning\": f\"Error: {str(e)}\",\n",
    "        }\n",
    "\n",
    "\n",
    "# Test dengan teks yang JELAS BUKAN hate speech (FIXED - no double output)\n",
    "test_texts = [\n",
    "    \"palang pintu pelintasan terbuka saat ka malioboro ekspres melintas kecelakaan pun tak bisa dihindari\",\n",
    "    \"selamat pagi semua, semoga hari ini menyenangkan\",\n",
    "    \"dasar orang hitam jelek kayak monyet\",  # ini baru hate speech\n",
    "]\n",
    "\n",
    "print(\"Testing full JSON output:\")\n",
    "for i, text in enumerate(test_texts):\n",
    "    result = classify_hate_speech_detailed(text)\n",
    "    print(f\"\\n=== Text {i+1} ===\")\n",
    "    print(f\"Text: {text[:50]}...\")\n",
    "    print(\"Full JSON Result:\")\n",
    "    print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206033a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test klasifikasi dengan 3 data pertama\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=== Testing dengan 3 data pertama ===\")\n",
    "test_data = data.head(3).copy()\n",
    "\n",
    "# Tambahkan kolom-kolom hasil klasifikasi\n",
    "classification_columns = [\n",
    "    \"HS\",\n",
    "    \"HS_INDIVIDU\",\n",
    "    \"HS_KELOMPOK\",\n",
    "    \"HS_RAS\",\n",
    "    \"HS_GENDER\",\n",
    "    \"HS_AGAMA\",\n",
    "    \"HS_POLITIK\",\n",
    "    \"HS_FISIK\",\n",
    "    \"HS_SOSIAL\",\n",
    "    \"confidence\",\n",
    "    \"reasoning\",\n",
    "]\n",
    "\n",
    "for col in classification_columns:\n",
    "    test_data[col] = None\n",
    "\n",
    "# Klasifikasi 3 data pertama\n",
    "for idx, row in test_data.iterrows():\n",
    "    print(f\"\\nüîÑ Memproses data ke-{idx+1}...\")\n",
    "    text = row[\"full_text\"]\n",
    "    print(f\"Text: {text[:100]}...\")\n",
    "\n",
    "    # Klasifikasi\n",
    "    result = classify_hate_speech_detailed(text)\n",
    "\n",
    "    # Simpan hasil ke dataframe\n",
    "    for col in classification_columns:\n",
    "        test_data.at[idx, col] = result.get(\n",
    "            col, 0 if col.startswith(\"HS\") else \"unknown\"\n",
    "        )\n",
    "\n",
    "    print(f\"HS: {result['HS']}, Confidence: {result['confidence']}\")\n",
    "\n",
    "    # Delay untuk menghindari overload\n",
    "    time.sleep(2)\n",
    "\n",
    "print(\"\\n=== Hasil Testing 3 Data ===\")\n",
    "display_fullscreen_wrap(\n",
    "    test_data[\n",
    "        [\"full_text\", \"HS\", \"HS_INDIVIDU\", \"HS_KELOMPOK\", \"confidence\", \"reasoning\"]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b448397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setelah testing berhasil, proses semua data\n",
    "print(\"=== Memproses SEMUA data ===\")\n",
    "print(f\"Total data yang akan diproses: {len(data)}\")\n",
    "\n",
    "# Buat copy data dengan kolom klasifikasi\n",
    "final_data = data.copy()\n",
    "\n",
    "# Tambahkan kolom hasil klasifikasi\n",
    "classification_columns = [\n",
    "    \"HS\",\n",
    "    \"HS_INDIVIDU\",\n",
    "    \"HS_KELOMPOK\",\n",
    "    \"HS_RAS\",\n",
    "    \"HS_GENDER\",\n",
    "    \"HS_AGAMA\",\n",
    "    \"HS_POLITIK\",\n",
    "    \"HS_FISIK\",\n",
    "    \"HS_SOSIAL\",\n",
    "    \"confidence\",\n",
    "    \"reasoning\",\n",
    "]\n",
    "\n",
    "for col in classification_columns:\n",
    "    final_data[col] = None\n",
    "\n",
    "# Progress tracking\n",
    "total_data = len(final_data)\n",
    "processed = 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"üöÄ Mulai klasifikasi semua data...\")\n",
    "\n",
    "for idx, row in final_data.iterrows():\n",
    "    processed += 1\n",
    "    text = row[\"full_text\"]\n",
    "\n",
    "    # Show progress\n",
    "    if processed % 10 == 0 or processed <= 5:\n",
    "        elapsed = time.time() - start_time\n",
    "        avg_time = elapsed / processed\n",
    "        remaining = (total_data - processed) * avg_time\n",
    "        print(\n",
    "            f\"Progress: {processed}/{total_data} ({processed/total_data*100:.1f}%) - ETA: {remaining/60:.1f} menit\"\n",
    "        )\n",
    "        print(f\"Current text: {text[:80]}...\")\n",
    "\n",
    "    # Klasifikasi\n",
    "    result = classify_hate_speech_detailed(text)\n",
    "\n",
    "    # Simpan hasil\n",
    "    for col in classification_columns:\n",
    "        final_data.at[idx, col] = result.get(\n",
    "            col, 0 if col.startswith(\"HS\") else \"unknown\"\n",
    "        )\n",
    "\n",
    "    # Delay untuk stability\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"\\n‚úÖ Selesai! Total waktu: {(time.time() - start_time)/60:.1f} menit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f9ef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan hasil klasifikasi\n",
    "import os\n",
    "\n",
    "# Buat folder output jika belum ada\n",
    "output_dir = \"data/diy\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Simpan ke Excel\n",
    "output_file = f\"{output_dir}/hate_speech_classification_results.xlsx\"\n",
    "final_data.to_excel(output_file, index=False)\n",
    "print(f\"‚úÖ Data berhasil disimpan ke: {output_file}\")\n",
    "\n",
    "# Simpan juga ke CSV untuk backup\n",
    "csv_file = f\"{output_dir}/hate_speech_classification_results.csv\"\n",
    "final_data.to_csv(csv_file, index=False)\n",
    "print(f\"‚úÖ Backup CSV disimpan ke: {csv_file}\")\n",
    "\n",
    "# Show summary statistics\n",
    "print(\"\\n=== SUMMARY HASIL KLASIFIKASI ===\")\n",
    "print(f\"Total data: {len(final_data)}\")\n",
    "print(\n",
    "    f\"Hate Speech terdeteksi: {final_data['HS'].sum()} ({final_data['HS'].sum()/len(final_data)*100:.1f}%)\"\n",
    ")\n",
    "print(\n",
    "    f\"Non Hate Speech: {len(final_data) - final_data['HS'].sum()} ({(len(final_data) - final_data['HS'].sum())/len(final_data)*100:.1f}%)\"\n",
    ")\n",
    "\n",
    "print(\"\\n=== Breakdown per kategori ===\")\n",
    "for col in [\n",
    "    \"HS_INDIVIDU\",\n",
    "    \"HS_KELOMPOK\",\n",
    "    \"HS_RAS\",\n",
    "    \"HS_GENDER\",\n",
    "    \"HS_AGAMA\",\n",
    "    \"HS_POLITIK\",\n",
    "    \"HS_FISIK\",\n",
    "    \"HS_SOSIAL\",\n",
    "]:\n",
    "    count = final_data[col].sum()\n",
    "    print(f\"{col}: {count} ({count/len(final_data)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n=== Confidence level ===\")\n",
    "print(final_data[\"confidence\"].value_counts())\n",
    "\n",
    "# Show sample results\n",
    "print(\"\\n=== Sample Hasil Klasifikasi ===\")\n",
    "display_fullscreen_wrap(\n",
    "    final_data[\n",
    "        [\"full_text\", \"HS\", \"HS_INDIVIDU\", \"HS_KELOMPOK\", \"confidence\", \"reasoning\"]\n",
    "    ].head(10)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
