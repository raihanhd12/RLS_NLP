{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63357a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import itables\n",
    "\n",
    "class getConnectorDatabase:\n",
    "    def __init__(self, gateway_url, service_slug, api_key=None):\n",
    "        self.gateway_url = gateway_url.rstrip(\"/\")  # Remove trailing slash\n",
    "        self.service_slug = service_slug\n",
    "\n",
    "        self.api_key = api_key\n",
    "        self.headers = {}\n",
    "\n",
    "        # Set API key in headers if provided\n",
    "        if self.api_key:\n",
    "            self.headers[\"x-api-key\"] = self.api_key\n",
    "            self.headers[\"Content-Type\"] = \"application/json\"\n",
    "\n",
    "    def build_url(self, endpoint_slug):\n",
    "        \"\"\"Build complete URL from components\"\"\"\n",
    "        return f\"{self.gateway_url}/{self.service_slug}/{endpoint_slug}\"\n",
    "\n",
    "    def get_connection(self, params=None, pretty_print=True):\n",
    "        \"\"\"Get data from specific endpoint\"\"\"\n",
    "        url = self.build_url(\"get-all-connector\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                url, headers=self.headers, params=params, timeout=30\n",
    "            )\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "\n",
    "                if pretty_print:\n",
    "                    print(json.dumps(data, indent=4, ensure_ascii=False))\n",
    "\n",
    "                return data\n",
    "            else:\n",
    "                raise Exception(f\"API Error: {response.status_code} - {response.text}\")\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise Exception(f\"Connection Error: {str(e)}\")\n",
    "\n",
    "    def execute_query(self, connector_id, query, params=None, max_rows=1000):\n",
    "        \"\"\"Execute SQL query on the database\"\"\"\n",
    "        url = self.build_url(f\"query-sql?connector_id={connector_id}\")\n",
    "\n",
    "        payload = {\"query\": query, \"params\": params or [], \"max_rows\": max_rows}\n",
    "\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                url, headers=self.headers, json=payload, timeout=30\n",
    "            )\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            else:\n",
    "                raise Exception(\n",
    "                    f\"Query Error: {response.status_code} - {response.text}\"\n",
    "                )\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            raise Exception(f\"Connection Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49b5d7e",
   "metadata": {},
   "source": [
    "# Define Connector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd34ec7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage Example\n",
    "connector = getConnectorDatabase(\n",
    "    gateway_url=\"https://ai.admasolusi.space\",\n",
    "    service_slug=\"connector-service\",\n",
    "    api_key=\"c1b311a30e10b6fc1b0137576667b024ced36054e3603ce36ce5be2b645f4128\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4170dfc6",
   "metadata": {},
   "source": [
    "### List All Connector connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9949ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from specific endpoint\n",
    "data = connector.get_connection(pretty_print=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13554c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Database:\n",
    "    def __init__(self, connector, connector_id):\n",
    "        self.connector = connector\n",
    "        self.connector_id = connector_id\n",
    "\n",
    "    def view_table_interactive(self, data, limit=50, height=\"400px\", title=None):\n",
    "        \"\"\"\n",
    "        Display database table or DataFrame using itables with fixed height and interactive features\n",
    "\n",
    "        Args:\n",
    "            data: Either table name (str) or DataFrame\n",
    "            limit (int): Maximum number of rows to display (only for table names)\n",
    "            height (str): Fixed height for the table container\n",
    "            title (str): Optional title override\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Case 1: data is a DataFrame\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                df = data\n",
    "                display_title = title or \"DataFrame Results\"\n",
    "\n",
    "                # Display info\n",
    "                print(\n",
    "                    f\"üìã {display_title} | üìä {len(df)} rows | üìà {len(df.columns)} cols\"\n",
    "                )\n",
    "\n",
    "                # Show sentiment distribution if sentiment columns exist\n",
    "                if \"sentiment_label\" in df.columns:\n",
    "                    print(\"\\nüìä Sentiment Distribution:\")\n",
    "                    sentiment_counts = df[\"sentiment_label\"].value_counts()\n",
    "                    for sentiment, count in sentiment_counts.items():\n",
    "                        percentage = (count / len(df)) * 100\n",
    "                        print(\n",
    "                            f\"  {sentiment.capitalize()}: {count} ({percentage:.1f}%)\"\n",
    "                        )\n",
    "\n",
    "                # Display with itables - LEFT ALIGNED\n",
    "                itables.show(\n",
    "                    df,\n",
    "                    lengthMenu=[10, 25, 50, 100],\n",
    "                    scrollY=height,\n",
    "                    scrollX=True,\n",
    "                    paging=True,\n",
    "                    maxBytes=10**6,\n",
    "                    maxColumns=50,\n",
    "                    style=\"text-align: left;\",  # Add this for left alignment\n",
    "                    classes=\"display\",\n",
    "                    columnDefs=[\n",
    "                        {\"className\": \"dt-left\", \"targets\": \"_all\"}  # Force left alignment for all columns\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            # Case 2: data is a table name (string)\n",
    "            elif isinstance(data, str):\n",
    "                table_name = data\n",
    "\n",
    "                # Get total row count and column info\n",
    "                count_query = f\"SELECT COUNT(*) as total_rows FROM {table_name}\"\n",
    "                count_result = self.connector.execute_query(\n",
    "                    self.connector_id, count_query\n",
    "                )\n",
    "\n",
    "                # Get column count\n",
    "                col_query = f\"\"\"\n",
    "                SELECT COUNT(*) as total_cols\n",
    "                FROM information_schema.columns\n",
    "                WHERE table_name = '{table_name}'\n",
    "                \"\"\"\n",
    "                col_result = self.connector.execute_query(self.connector_id, col_query)\n",
    "\n",
    "                # Get table data\n",
    "                query = f\"SELECT * FROM {table_name} LIMIT {limit}\"\n",
    "                result = self.connector.execute_query(\n",
    "                    self.connector_id, query, max_rows=limit\n",
    "                )\n",
    "\n",
    "                if (\n",
    "                    result[\"success\"]\n",
    "                    and count_result[\"success\"]\n",
    "                    and col_result[\"success\"]\n",
    "                ):\n",
    "                    df = pd.DataFrame(result[\"rows\"])\n",
    "\n",
    "                    # Get actual totals\n",
    "                    total_rows = count_result[\"rows\"][0][\"total_rows\"]\n",
    "                    total_cols = col_result[\"rows\"][0][\"total_cols\"]\n",
    "\n",
    "                    # Display info with actual table size vs displayed size\n",
    "                    display_title = title or table_name\n",
    "                    print(\n",
    "                        f\"üìã {display_title} | üìä {total_rows} total rows ({len(df)} displayed) | üìà {total_cols} cols | ‚ö° {result['execution_time_ms']}ms\"\n",
    "                    )\n",
    "\n",
    "                    # Display with itables - LEFT ALIGNED\n",
    "                    itables.show(\n",
    "                        df,\n",
    "                        lengthMenu=[10, 25, 50, 100],\n",
    "                        scrollY=height,\n",
    "                        scrollX=True,\n",
    "                        paging=True,\n",
    "                        maxBytes=10**6,\n",
    "                        maxColumns=50,\n",
    "                        style=\"text-align: left;\",  # Add this for left alignment\n",
    "                        classes=\"display\",\n",
    "                        columnDefs=[\n",
    "                            {\"className\": \"dt-left\", \"targets\": \"_all\"}  # Force left alignment for all columns\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    print(f\"‚ùå Error: {result.get('message', 'Unknown error')}\")\n",
    "            else:\n",
    "                print(\n",
    "                    f\"‚ùå Error: Invalid data type. Expected DataFrame or table name string.\"\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {str(e)}\")\n",
    "\n",
    "    def get_table_list(self):\n",
    "        \"\"\"Get all available tables\"\"\"\n",
    "        query = \"SELECT table_name FROM information_schema.tables WHERE table_schema = 'public'\"\n",
    "        result = self.connector.execute_query(self.connector_id, query)\n",
    "\n",
    "        if result[\"success\"]:\n",
    "            tables = [row[\"table_name\"] for row in result[\"rows\"]]\n",
    "            print(\"üìã Available Tables:\")\n",
    "            for i, table in enumerate(tables, 1):\n",
    "                print(f\"{i}. {table}\")\n",
    "            return tables\n",
    "        else:\n",
    "            print(f\"‚ùå Error: {result.get('message', 'Unknown error')}\")\n",
    "\n",
    "    def describe_table(self, table_name):\n",
    "        \"\"\"Get detailed table structure\"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT column_name, data_type, is_nullable, column_default\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_name = '{table_name}'\n",
    "        ORDER BY ordinal_position\n",
    "        \"\"\"\n",
    "        result = self.connector.execute_query(self.connector_id, query)\n",
    "\n",
    "        if result[\"success\"]:\n",
    "            df = pd.DataFrame(result[\"rows\"])\n",
    "            print(f\"üìã Table Structure: {table_name}\")\n",
    "            print(\"=\" * 60)\n",
    "            display(df)\n",
    "            return df\n",
    "        else:\n",
    "            print(f\"‚ùå Error: {result.get('message', 'Unknown error')}\")\n",
    "\n",
    "    def get_dataframe(self, table_name, limit=100, columns=None):\n",
    "        \"\"\"\n",
    "        Get DataFrame from table with optional column selection\n",
    "\n",
    "        Args:\n",
    "            table_name (str): Name of the table\n",
    "            limit (int): Number of rows to fetch\n",
    "            columns (list): Specific columns to select (optional)\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with the data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Build query\n",
    "            if columns:\n",
    "                columns_str = \", \".join(columns)\n",
    "                query = f\"SELECT {columns_str} FROM {table_name} LIMIT {limit}\"\n",
    "            else:\n",
    "                query = f\"SELECT * FROM {table_name} LIMIT {limit}\"\n",
    "\n",
    "            result = self.connector.execute_query(self.connector_id, query, max_rows=limit)\n",
    "\n",
    "            if result[\"success\"]:\n",
    "                df = pd.DataFrame(result[\"rows\"])\n",
    "                print(f\"‚úÖ Fetched {len(df)} rows from {table_name}\")\n",
    "                if columns:\n",
    "                    print(f\"üìä Selected columns: {list(df.columns)}\")\n",
    "                else:\n",
    "                    print(f\"üìä All columns: {list(df.columns)}\")\n",
    "                return df\n",
    "            else:\n",
    "                print(f\"‚ùå Error: {result.get('message', 'Unknown error')}\")\n",
    "                return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def get_columns(self, table_name):\n",
    "        \"\"\"\n",
    "        Get all column names from a table\n",
    "\n",
    "        Args:\n",
    "            table_name (str): Name of the table\n",
    "\n",
    "        Returns:\n",
    "            list: List of column names\n",
    "        \"\"\"\n",
    "        query = f\"\"\"\n",
    "        SELECT column_name\n",
    "        FROM information_schema.columns\n",
    "        WHERE table_name = '{table_name}'\n",
    "        ORDER BY ordinal_position\n",
    "        \"\"\"\n",
    "        result = self.connector.execute_query(self.connector_id, query)\n",
    "\n",
    "        if result[\"success\"]:\n",
    "            columns = [row[\"column_name\"] for row in result[\"rows\"]]\n",
    "            print(f\"üìã Columns in {table_name}: {columns}\")\n",
    "            return columns\n",
    "        else:\n",
    "            print(f\"‚ùå Error getting columns: {result.get('message', 'Unknown error')}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022f1f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "connector_id = \"a46449c2-3b09-453f-b0ee-381c91da9779\"\n",
    "db = Database(connector, connector_id)\n",
    "tables = db.get_table_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b567c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.view_table_interactive(\"sample_data3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be1a2a7",
   "metadata": {},
   "source": [
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0899d25",
   "metadata": {},
   "source": [
    "### 1. IndoNLP Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8962326a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from indoNLP.preprocessing import (\n",
    "    remove_html,\n",
    "    remove_url,\n",
    "    remove_stopwords,\n",
    "    replace_slang,\n",
    "    replace_word_elongation,\n",
    "    emoji_to_words,\n",
    "    pipeline,\n",
    ")\n",
    "# 1. Setup pipeline\n",
    "nlp_pipeline = pipeline(\n",
    "    [remove_html, remove_url, emoji_to_words, replace_slang, replace_word_elongation]\n",
    ")\n",
    "\n",
    "# 2. Ambil data\n",
    "df = db.get_dataframe(\"sample_data3\", limit=500)\n",
    "\n",
    "\n",
    "# 3. Simple preprocessing function\n",
    "def clean_text(text):\n",
    "    if pd.isna(text) or text is None or text == \"\":\n",
    "        return \"No Comment\"\n",
    "    try:\n",
    "        return nlp_pipeline(str(text))\n",
    "    except:\n",
    "        return str(text)\n",
    "\n",
    "\n",
    "# 4. Apply preprocessing\n",
    "print(\"üîÑ Preprocessing...\")\n",
    "df[\"full_text_clean\"] = df[\"full_text\"].apply(clean_text)\n",
    "\n",
    "# 5. Lihat hasil\n",
    "print(\"‚úÖ Done!\")\n",
    "print(f\"üìä Processed {len(df)} rows\")\n",
    "\n",
    "# Show examples\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original:  {df['full_text'].iloc[i]}\")\n",
    "    print(f\"Processed: {df['full_text_clean'].iloc[i]}\")\n",
    "\n",
    "# 6. View interactive\n",
    "db.view_table_interactive(df[[\"full_text\", \"full_text_clean\"]], limit=50, title=\"Processed Comments\", height=\"400px\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd94db47",
   "metadata": {},
   "source": [
    "### 2. Twitter Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dee53f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class TwitterCleaner:\n",
    "    @staticmethod  # Add this decorator\n",
    "    def advanced_social_media_cleaning(text):\n",
    "        \"\"\"\n",
    "        More advanced social media cleaning\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or text is None or text == \"\":\n",
    "            return \"No Comment\"\n",
    "\n",
    "        try:\n",
    "            text = str(text)\n",
    "\n",
    "            # 1. Remove URLs (more comprehensive)\n",
    "            text = re.sub(\n",
    "                r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n",
    "                \"\",\n",
    "                text,\n",
    "            )\n",
    "            text = re.sub(\n",
    "                r\"www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n",
    "                \"\",\n",
    "                text,\n",
    "            )\n",
    "\n",
    "            # 2. Remove mentions and replies\n",
    "            text = re.sub(r\"@\\w+\", \"\", text)\n",
    "\n",
    "            # 3. Clean hashtags (remove # but keep the word)\n",
    "            text = re.sub(r\"#(\\w+)\", r\"\\1\", text)\n",
    "\n",
    "            # 4. Remove retweet markers\n",
    "            text = re.sub(r\"^RT\\s*:?\\s*\", \"\", text, flags=re.IGNORECASE)\n",
    "            text = re.sub(r\"\\bRT\\b\", \"\", text, flags=re.IGNORECASE)\n",
    "\n",
    "            # 5. Remove quote tweet markers\n",
    "            text = re.sub(r'^\".*\"', \"\", text)\n",
    "\n",
    "            # 6. Clean excessive punctuation\n",
    "            text = re.sub(r\"[!]{2,}\", \"!\", text)\n",
    "            text = re.sub(r\"[?]{2,}\", \"?\", text)\n",
    "            text = re.sub(r\"[.]{3,}\", \"...\", text)\n",
    "\n",
    "            # 7. Remove special characters but keep important punctuation\n",
    "            text = re.sub(r\"[^\\w\\s!?.,\\-]\", \" \", text)\n",
    "\n",
    "            # 8. Fix spacing issues\n",
    "            text = re.sub(r\"\\s+\", \" \", text)\n",
    "            text = re.sub(r\"\\n+\", \" \", text)\n",
    "\n",
    "            # 9. Convert to lowercase\n",
    "            text = text.lower()\n",
    "\n",
    "            # 10. Remove leading/trailing whitespace\n",
    "            text = text.strip()\n",
    "\n",
    "            return text if text else \"No Comment\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in advanced cleaning: {e}\")\n",
    "            return str(text)\n",
    "\n",
    "    # Now apply Twitter cleaning to your IndoNLP cleaned text\n",
    "print(\"üîÑ Applying Twitter cleaning to IndoNLP processed text...\")\n",
    "\n",
    "# Apply Twitter cleaning to the already cleaned text\n",
    "df[\"full_text_twitter_clean\"] = df[\"full_text_clean\"].apply(\n",
    "    TwitterCleaner.advanced_social_media_cleaning\n",
    ")\n",
    "\n",
    "db.view_table_interactive(\n",
    "    df[[\"full_text_clean\", \"full_text_twitter_clean\"]],\n",
    "    limit=50,\n",
    "    title=\"IndoNLP vs Twitter Cleaning\",\n",
    "    height=\"400px\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2679b4",
   "metadata": {},
   "source": [
    "# Model Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622d9045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd48af27",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'humanize'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HfApi\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhumanize\u001b[39;00m  \u001b[38;5;66;03m# Untuk format angka (pip install humanize)\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minspect_model_details\u001b[39m(model_names):\n\u001b[32m      6\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[33;03m    Inspect multiple models for a comprehensive comparison, including\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[33;03m    architecture, tokenizer, and Hub metadata.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'humanize'"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import humanize  # Untuk format angka (pip install humanize)\n",
    "\n",
    "\n",
    "def inspect_model_details(model_names):\n",
    "    \"\"\"\n",
    "    Inspect multiple models for a comprehensive comparison, including\n",
    "    architecture, tokenizer, and Hub metadata.\n",
    "    \"\"\"\n",
    "    print(\"üîç COMPREHENSIVE MODEL INSPECTION\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Inisialisasi API untuk mengambil data dari Hugging Face Hub\n",
    "    hf_api = HfApi()\n",
    "    results = {}\n",
    "\n",
    "    for model_name in model_names:\n",
    "        print(f\"\\nüìã Inspecting: {model_name}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        try:\n",
    "            # 1. Inspeksi Konfigurasi (Seperti kode Anda, dengan tambahan)\n",
    "            config = AutoConfig.from_pretrained(model_name)\n",
    "            info = {\n",
    "                \"model_type\": config.model_type,\n",
    "                # Detail Arsitektur\n",
    "                \"hidden_size\": getattr(config, \"hidden_size\", \"N/A\"),\n",
    "                \"num_layers\": getattr(config, \"num_hidden_layers\", \"N/A\"),\n",
    "                \"num_heads\": getattr(config, \"num_attention_heads\", \"N/A\"),\n",
    "                \"num_parameters\": (\n",
    "                    humanize.intword(config.num_parameters())\n",
    "                    if hasattr(config, \"num_parameters\")\n",
    "                    and callable(config.num_parameters)\n",
    "                    else \"N/A\"\n",
    "                ),\n",
    "                # Detail Klasifikasi\n",
    "                \"num_labels\": config.num_labels,\n",
    "                \"labels\": dict(config.id2label) if hasattr(config, \"id2label\") else {},\n",
    "                \"problem_type\": getattr(config, \"problem_type\", \"Not specified\"),\n",
    "            }\n",
    "\n",
    "            print(\"   [Architecture]\")\n",
    "            print(f\"   - Model Type: {info['model_type']}\")\n",
    "            print(f\"   - Parameters: {info['num_parameters']}\")\n",
    "            print(\n",
    "                f\"   - Layers: {info['num_layers']}, Hidden Size: {info['hidden_size']}, Heads: {info['num_heads']}\"\n",
    "            )\n",
    "\n",
    "            print(\"\\n   [Classification Task]\")\n",
    "            print(f\"   - Problem Type: {info['problem_type']}\")\n",
    "            print(f\"   - Number of Labels: {info['num_labels']}\")\n",
    "            if info[\"labels\"]:\n",
    "                print(f\"   - Categories: {list(info['labels'].values())}\")\n",
    "\n",
    "            # 2. Inspeksi Tokenizer\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            info[\"tokenizer_class\"] = tokenizer.__class__.__name__\n",
    "            info[\"vocab_size\"] = humanize.intword(tokenizer.vocab_size)\n",
    "\n",
    "            print(\"\\n   [Tokenizer]\")\n",
    "            print(f\"   - Class: {info['tokenizer_class']}\")\n",
    "            print(f\"   - Vocabulary Size: {info['vocab_size']}\")\n",
    "\n",
    "            # 3. Inspeksi Metadata dari Hugging Face Hub\n",
    "            model_info_hub = hf_api.model_info(model_name)\n",
    "            info[\"downloads\"] = humanize.intword(model_info_hub.downloads)\n",
    "            info[\"likes\"] = humanize.intword(model_info_hub.likes)\n",
    "            info[\"last_modified\"] = model_info_hub.lastModified.split(\"T\")[0]\n",
    "\n",
    "            print(\"\\n   [Hub Info]\")\n",
    "            print(f\"   - Downloads: {info['downloads']}\")\n",
    "            print(f\"   - Likes: {info['likes']}\")\n",
    "            print(f\"   - Last Modified: {info['last_modified']}\")\n",
    "\n",
    "            results[model_name] = info\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error processing {model_name}: {e}\")\n",
    "            results[model_name] = {\"error\": str(e)}\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2561936",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_compare = [\n",
    "    \"PaceKW/indobert-base-p1-multilabel-indonesian-hate-speech-new\",\n",
    "    \"Aardiiiiy/indobertweet-base-Indonesian-sentiment-analysis\",\n",
    "    \"Aardiiiiy/EmoSense-ID-Indonesian-Emotion-Classifier\",\n",
    "]\n",
    "\n",
    "detailed_results = inspect_model_details(models_to_compare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bf1a48",
   "metadata": {},
   "source": [
    "# Sentiment Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa8c057",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalyzer:\n",
    "    def __init__(\n",
    "        self, model_name=\"Aardiiiiy/indobertweet-base-Indonesian-sentiment-analysis\"\n",
    "    ):\n",
    "        \"\"\"Initialize sentiment analyzer with IndoBERTweet model\"\"\"\n",
    "        print(\"üîÑ Loading IndoBERTweet sentiment model...\")\n",
    "\n",
    "        # Using pipeline (simplest approach)\n",
    "        self.pipe = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=model_name,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "        )\n",
    "\n",
    "        # Load tokenizer and model separately for more control if needed\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "        # Based on our inspection - we know exactly what labels exist\n",
    "        self.sentiment_labels = [\"NEGATIVE\", \"NEUTRAL\", \"POSITIVE\"]\n",
    "\n",
    "        print(\"‚úÖ Sentiment model loaded successfully!\")\n",
    "        print(f\"üîß Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "        print(f\"üè∑Ô∏è Labels: {', '.join(self.sentiment_labels)}\")\n",
    "\n",
    "    def predict_single(self, text):\n",
    "        \"\"\"Predict sentiment for a single text\"\"\"\n",
    "        if pd.isna(text) or text is None or text == \"\" or text == \"No Comment\":\n",
    "            return {\"label\": \"NEUTRAL\", \"score\": 0.0, \"confidence\": \"low\"}\n",
    "\n",
    "        try:\n",
    "            result = self.pipe(str(text))\n",
    "            prediction = result[0]\n",
    "\n",
    "            # Add confidence level based on score\n",
    "            if prediction[\"score\"] >= 0.8:\n",
    "                confidence = \"high\"\n",
    "            elif prediction[\"score\"] >= 0.6:\n",
    "                confidence = \"medium\"\n",
    "            else:\n",
    "                confidence = \"low\"\n",
    "\n",
    "            return {\n",
    "                \"label\": prediction[\"label\"],\n",
    "                \"score\": prediction[\"score\"],\n",
    "                \"confidence\": confidence,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting sentiment: {e}\")\n",
    "            return {\"label\": \"NEUTRAL\", \"score\": 0.0, \"confidence\": \"error\"}\n",
    "\n",
    "    def predict_batch(self, texts, batch_size=32):\n",
    "        \"\"\"Predict sentiment for multiple texts efficiently\"\"\"\n",
    "        results = []\n",
    "\n",
    "        # Convert to list if pandas Series\n",
    "        if hasattr(texts, \"tolist\"):\n",
    "            texts = texts.tolist()\n",
    "\n",
    "        print(f\"üîÑ Processing {len(texts)} texts for sentiment analysis...\")\n",
    "        print(f\"üìä Batch size: {batch_size}\")\n",
    "\n",
    "        # Process in batches with progress bar\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Analyzing sentiment\"):\n",
    "            batch = texts[i : i + batch_size]\n",
    "\n",
    "            # Clean batch texts\n",
    "            clean_batch = []\n",
    "            for text in batch:\n",
    "                if pd.isna(text) or text is None or text == \"\" or text == \"No Comment\":\n",
    "                    clean_batch.append(\"No Comment\")\n",
    "                else:\n",
    "                    clean_batch.append(str(text))\n",
    "\n",
    "            try:\n",
    "                # Predict batch\n",
    "                batch_results = self.pipe(clean_batch)\n",
    "\n",
    "                # Process results\n",
    "                for j, result in enumerate(batch_results):\n",
    "                    if clean_batch[j] == \"No Comment\":\n",
    "                        results.append(\n",
    "                            {\"label\": \"NEUTRAL\", \"score\": 0.0, \"confidence\": \"low\"}\n",
    "                        )\n",
    "                    else:\n",
    "                        # Add confidence level\n",
    "                        if result[\"score\"] >= 0.8:\n",
    "                            confidence = \"high\"\n",
    "                        elif result[\"score\"] >= 0.6:\n",
    "                            confidence = \"medium\"\n",
    "                        else:\n",
    "                            confidence = \"low\"\n",
    "\n",
    "                        results.append(\n",
    "                            {\n",
    "                                \"label\": result[\"label\"],\n",
    "                                \"score\": result[\"score\"],\n",
    "                                \"confidence\": confidence,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in sentiment batch {i//batch_size + 1}: {e}\")\n",
    "                # Add neutral predictions for failed batch\n",
    "                for _ in range(len(batch)):\n",
    "                    results.append(\n",
    "                        {\"label\": \"NEUTRAL\", \"score\": 0.0, \"confidence\": \"error\"}\n",
    "                    )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def analyze_results(self, results):\n",
    "        \"\"\"Analyze and display sentiment analysis results\"\"\"\n",
    "        total = len(results)\n",
    "\n",
    "        print(f\"\\nüìä SENTIMENT ANALYSIS SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"üìù Total texts: {total}\")\n",
    "\n",
    "        # Sentiment distribution\n",
    "        sentiment_counts = {\"POSITIVE\": 0, \"NEGATIVE\": 0, \"NEUTRAL\": 0}\n",
    "        for result in results:\n",
    "            label = result[\"label\"]\n",
    "            sentiment_counts[label] = sentiment_counts.get(label, 0) + 1\n",
    "\n",
    "        print(f\"\\nüí≠ Sentiment Distribution:\")\n",
    "        for sentiment, count in sentiment_counts.items():\n",
    "            percentage = (count / total) * 100\n",
    "            emoji = (\n",
    "                \"üòä\"\n",
    "                if sentiment == \"POSITIVE\"\n",
    "                else \"üòû\" if sentiment == \"NEGATIVE\" else \"üòê\"\n",
    "            )\n",
    "            print(f\"   {emoji} {sentiment}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "        # Confidence distribution\n",
    "        conf_counts = {\"high\": 0, \"medium\": 0, \"low\": 0, \"error\": 0}\n",
    "        for result in results:\n",
    "            conf_counts[result[\"confidence\"]] += 1\n",
    "\n",
    "        print(f\"\\nüéØ Confidence Distribution:\")\n",
    "        for conf, count in conf_counts.items():\n",
    "            percentage = (count / total) * 100\n",
    "            print(f\"   {conf.capitalize()}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "        return {\n",
    "            \"total\": total,\n",
    "            \"sentiment_distribution\": sentiment_counts,\n",
    "            \"confidence_distribution\": conf_counts,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ac7100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SentimentAnalyzer only\n",
    "\n",
    "print(\"üöÄ Testing SentimentAnalyzer...\")\n",
    "\n",
    "# Initialize\n",
    "try:\n",
    "    sentiment_analyzer = SentimentAnalyzer()\n",
    "\n",
    "    # Test with some examples\n",
    "    test_texts = [\n",
    "        \"Selamat pagi semua! Hari ini sangat menyenangkan\",\n",
    "        \"Pelayanan buruk sekali, mengecewakan banget\",\n",
    "        \"Biasa saja, tidak ada yang istimewa\",\n",
    "        \"Makanannya enak banget, sangat memuaskan!\",\n",
    "        \"Harganya terlalu mahal untuk kualitas begini\",\n",
    "        \"Terima kasih banyak atas bantuan Anda\",\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nüß™ Testing individual sentiment predictions:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for text in test_texts:\n",
    "        result = sentiment_analyzer.predict_single(text)\n",
    "        sentiment_emoji = (\n",
    "            \"üòä\"\n",
    "            if result[\"label\"] == \"POSITIVE\"\n",
    "            else \"üòû\" if result[\"label\"] == \"NEGATIVE\" else \"üòê\"\n",
    "        )\n",
    "        print(f\"Text: '{text}'\")\n",
    "        print(\n",
    "            f\"   üí≠ Sentiment: {sentiment_emoji} {result['label']} ({result['score']:.3f}) - {result['confidence']}\"\n",
    "        )\n",
    "        print()\n",
    "\n",
    "    print(\"‚úÖ SentimentAnalyzer test completed!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in SentimentAnalyzer: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad4fcf",
   "metadata": {},
   "source": [
    "# Emotion Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328c2753",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionAnalyzer:\n",
    "    def __init__(\n",
    "        self, model_name=\"Aardiiiiy/EmoSense-ID-Indonesian-Emotion-Classifier\"\n",
    "    ):\n",
    "        \"\"\"Initialize emotion analyzer with EmoSense model\"\"\"\n",
    "        print(\"üîÑ Loading EmoSense Indonesian emotion model...\")\n",
    "\n",
    "        # Using pipeline (simplest approach)\n",
    "        self.pipe = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=model_name,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "        )\n",
    "\n",
    "        # Load tokenizer and model separately for more control if needed\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "        # Based on our inspection - we know exactly what labels exist (Plutchik's 8 emotions)\n",
    "        self.emotion_labels = [\n",
    "            \"Anger\",\n",
    "            \"Anticipation\",\n",
    "            \"Disgust\",\n",
    "            \"Fear\",\n",
    "            \"Joy\",\n",
    "            \"Sadness\",\n",
    "            \"Surprise\",\n",
    "            \"Trust\",\n",
    "        ]\n",
    "\n",
    "        # Emotion emojis for better display\n",
    "        self.emotion_emojis = {\n",
    "            \"Anger\": \"üò°\",\n",
    "            \"Anticipation\": \"ü§î\",\n",
    "            \"Disgust\": \"ü§¢\",\n",
    "            \"Fear\": \"üò®\",\n",
    "            \"Joy\": \"üòä\",\n",
    "            \"Sadness\": \"üò¢\",\n",
    "            \"Surprise\": \"üò≤\",\n",
    "            \"Trust\": \"ü§ù\",\n",
    "        }\n",
    "\n",
    "        print(\"‚úÖ Emotion model loaded successfully!\")\n",
    "        print(f\"üîß Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "        print(f\"üé≠ Emotions: {', '.join(self.emotion_labels)}\")\n",
    "\n",
    "    def predict_single(self, text):\n",
    "        \"\"\"Predict emotion for a single text\"\"\"\n",
    "        if pd.isna(text) or text is None or text == \"\" or text == \"No Comment\":\n",
    "            return {\"label\": \"Trust\", \"score\": 0.0, \"confidence\": \"low\"}\n",
    "\n",
    "        try:\n",
    "            result = self.pipe(str(text))\n",
    "            prediction = result[0]\n",
    "\n",
    "            # Add confidence level based on score\n",
    "            if prediction[\"score\"] >= 0.8:\n",
    "                confidence = \"high\"\n",
    "            elif prediction[\"score\"] >= 0.6:\n",
    "                confidence = \"medium\"\n",
    "            else:\n",
    "                confidence = \"low\"\n",
    "\n",
    "            return {\n",
    "                \"label\": prediction[\"label\"],\n",
    "                \"score\": prediction[\"score\"],\n",
    "                \"confidence\": confidence,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting emotion: {e}\")\n",
    "            return {\"label\": \"Trust\", \"score\": 0.0, \"confidence\": \"error\"}\n",
    "\n",
    "    def predict_batch(self, texts, batch_size=32):\n",
    "        \"\"\"Predict emotion for multiple texts efficiently\"\"\"\n",
    "        results = []\n",
    "\n",
    "        # Convert to list if pandas Series\n",
    "        if hasattr(texts, \"tolist\"):\n",
    "            texts = texts.tolist()\n",
    "\n",
    "        print(f\"üîÑ Processing {len(texts)} texts for emotion analysis...\")\n",
    "        print(f\"üìä Batch size: {batch_size}\")\n",
    "\n",
    "        # Process in batches with progress bar\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Analyzing emotions\"):\n",
    "            batch = texts[i : i + batch_size]\n",
    "\n",
    "            # Clean batch texts\n",
    "            clean_batch = []\n",
    "            for text in batch:\n",
    "                if pd.isna(text) or text is None or text == \"\" or text == \"No Comment\":\n",
    "                    clean_batch.append(\"No Comment\")\n",
    "                else:\n",
    "                    clean_batch.append(str(text))\n",
    "\n",
    "            try:\n",
    "                # Predict batch\n",
    "                batch_results = self.pipe(clean_batch)\n",
    "\n",
    "                # Process results\n",
    "                for j, result in enumerate(batch_results):\n",
    "                    if clean_batch[j] == \"No Comment\":\n",
    "                        results.append(\n",
    "                            {\"label\": \"Trust\", \"score\": 0.0, \"confidence\": \"low\"}\n",
    "                        )\n",
    "                    else:\n",
    "                        # Add confidence level\n",
    "                        if result[\"score\"] >= 0.8:\n",
    "                            confidence = \"high\"\n",
    "                        elif result[\"score\"] >= 0.6:\n",
    "                            confidence = \"medium\"\n",
    "                        else:\n",
    "                            confidence = \"low\"\n",
    "\n",
    "                        results.append(\n",
    "                            {\n",
    "                                \"label\": result[\"label\"],\n",
    "                                \"score\": result[\"score\"],\n",
    "                                \"confidence\": confidence,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in emotion batch {i//batch_size + 1}: {e}\")\n",
    "                # Add default predictions for failed batch\n",
    "                for _ in range(len(batch)):\n",
    "                    results.append(\n",
    "                        {\"label\": \"Trust\", \"score\": 0.0, \"confidence\": \"error\"}\n",
    "                    )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def analyze_results(self, results):\n",
    "        \"\"\"Analyze and display emotion analysis results\"\"\"\n",
    "        total = len(results)\n",
    "\n",
    "        print(f\"\\nüìä EMOTION ANALYSIS SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"üìù Total texts: {total}\")\n",
    "\n",
    "        # Emotion distribution\n",
    "        emotion_counts = {}\n",
    "        for emotion in self.emotion_labels:\n",
    "            emotion_counts[emotion] = 0\n",
    "\n",
    "        for result in results:\n",
    "            label = result[\"label\"]\n",
    "            emotion_counts[label] = emotion_counts.get(label, 0) + 1\n",
    "\n",
    "        print(f\"\\nüé≠ Emotion Distribution:\")\n",
    "        # Sort by count, descending\n",
    "        sorted_emotions = sorted(\n",
    "            emotion_counts.items(), key=lambda x: x[1], reverse=True\n",
    "        )\n",
    "        for emotion, count in sorted_emotions:\n",
    "            percentage = (count / total) * 100\n",
    "            emoji = self.emotion_emojis.get(emotion, \"üé≠\")\n",
    "            print(f\"   {emoji} {emotion}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "        # Confidence distribution\n",
    "        conf_counts = {\"high\": 0, \"medium\": 0, \"low\": 0, \"error\": 0}\n",
    "        for result in results:\n",
    "            conf_counts[result[\"confidence\"]] += 1\n",
    "\n",
    "        print(f\"\\nüéØ Confidence Distribution:\")\n",
    "        for conf, count in conf_counts.items():\n",
    "            percentage = (count / total) * 100\n",
    "            print(f\"   {conf.capitalize()}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "        return {\n",
    "            \"total\": total,\n",
    "            \"emotion_distribution\": emotion_counts,\n",
    "            \"confidence_distribution\": conf_counts,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ee5480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test EmotionAnalyzer only\n",
    "\n",
    "print(\"üöÄ Testing EmotionAnalyzer...\")\n",
    "\n",
    "# Initialize\n",
    "try:\n",
    "    emotion_analyzer = EmotionAnalyzer()\n",
    "\n",
    "    # Test with emotion-specific examples\n",
    "    emotion_test_texts = [\n",
    "        (\"Saya sangat marah dengan pelayanan ini!\", \"Expected: Anger\"),\n",
    "        (\"Wah senang sekali dapat hadiah ini!\", \"Expected: Joy\"),\n",
    "        (\"Saya merasa sedih sekali hari ini\", \"Expected: Sadness\"),\n",
    "        (\"Ngeri banget nonton film horror tadi\", \"Expected: Fear\"),\n",
    "        (\"Kaget banget ternyata dia datang!\", \"Expected: Surprise\"),\n",
    "        (\"Jijik banget lihat yang begitu\", \"Expected: Disgust\"),\n",
    "        (\"Saya percaya sepenuhnya dengan tim ini\", \"Expected: Trust\"),\n",
    "        (\"Tidak sabar menunggu acara besok!\", \"Expected: Anticipation\"),\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nüß™ Testing individual emotion predictions:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for text, expected in emotion_test_texts:\n",
    "        result = emotion_analyzer.predict_single(text)\n",
    "        emotion_emoji = emotion_analyzer.emotion_emojis.get(result[\"label\"], \"üé≠\")\n",
    "        print(f\"Text: '{text}'\")\n",
    "        print(f\"   {expected}\")\n",
    "        print(\n",
    "            f\"   üé≠ Emotion: {emotion_emoji} {result['label']} ({result['score']:.3f}) - {result['confidence']}\"\n",
    "        )\n",
    "        print()\n",
    "\n",
    "    print(\"‚úÖ EmotionAnalyzer test completed!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in EmotionAnalyzer: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5a0f9e",
   "metadata": {},
   "source": [
    "# Hate speech Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ce28e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HateSpeechAnalyzer:\n",
    "    def __init__(\n",
    "        self, model_name=\"PaceKW/indobert-base-p1-multilabel-indonesian-hate-speech-new\"\n",
    "    ):\n",
    "        \"\"\"Initialize hate speech analyzer\"\"\"\n",
    "        print(\"üîÑ Loading Indonesian Hate Speech model...\")\n",
    "\n",
    "        # Using pipeline (simplest approach)\n",
    "        self.pipe = pipeline(\n",
    "            \"text-classification\",\n",
    "            model=model_name,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "            return_all_scores=True,  # Important for multilabel\n",
    "        )\n",
    "\n",
    "        # Load tokenizer and model separately for more control if needed\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "        # Based on our inspection - we know exactly what labels exist\n",
    "        self.hate_categories = [\n",
    "            \"HS\",\n",
    "            \"Abusive\",\n",
    "            \"HS_Individual\",\n",
    "            \"HS_Group\",\n",
    "            \"HS_Religion\",\n",
    "            \"HS_Race\",\n",
    "            \"HS_Physical\",\n",
    "            \"HS_Gender\",\n",
    "            \"HS_Other\",\n",
    "            \"HS_Weak\",\n",
    "            \"HS_Moderate\",\n",
    "            \"HS_Strong\",\n",
    "        ]\n",
    "\n",
    "        print(\"‚úÖ Hate speech model loaded successfully!\")\n",
    "        print(f\"üîß Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
    "        print(f\"üè∑Ô∏è Categories: {', '.join(self.hate_categories)}\")\n",
    "\n",
    "    def predict_single(self, text, threshold=0.5):\n",
    "        \"\"\"Predict hate speech for a single text\"\"\"\n",
    "        if pd.isna(text) or text is None or text == \"\" or text == \"No Comment\":\n",
    "            return {\n",
    "                \"is_hate_speech\": False,\n",
    "                \"categories\": [],\n",
    "                \"scores\": {},\n",
    "                \"max_score\": 0.0,\n",
    "                \"confidence\": \"low\",\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            # Get predictions for all labels\n",
    "            results = self.pipe(str(text))\n",
    "\n",
    "            # Debug: Print hasil untuk lihat struktur\n",
    "            print(f\"Debug - Raw result type: {type(results)}\")\n",
    "            print(f\"Debug - Raw result: {results}\")\n",
    "\n",
    "            # Handle different output formats\n",
    "            if isinstance(results, list):\n",
    "                # Jika hasil adalah list of lists (nested)\n",
    "                if len(results) > 0 and isinstance(results[0], list):\n",
    "                    predictions = results[0]  # Ambil list pertama\n",
    "                else:\n",
    "                    predictions = results  # Sudah format yang benar\n",
    "            else:\n",
    "                predictions = [results]  # Bungkus dalam list jika bukan list\n",
    "\n",
    "            # Process multilabel results\n",
    "            active_categories = []\n",
    "            all_scores = {}\n",
    "            max_score = 0.0\n",
    "\n",
    "            for prediction in predictions:\n",
    "                # Handle different key formats\n",
    "                if isinstance(prediction, dict):\n",
    "                    if \"label\" in prediction and \"score\" in prediction:\n",
    "                        label = prediction[\"label\"]\n",
    "                        score = prediction[\"score\"]\n",
    "                    elif \"LABEL\" in prediction and \"SCORE\" in prediction:\n",
    "                        label = prediction[\"LABEL\"]\n",
    "                        score = prediction[\"SCORE\"]\n",
    "                    else:\n",
    "                        print(f\"Debug - Unknown prediction format: {prediction}\")\n",
    "                        continue\n",
    "                else:\n",
    "                    print(f\"Debug - Unexpected prediction type: {type(prediction)}\")\n",
    "                    continue\n",
    "\n",
    "                all_scores[label] = score\n",
    "                max_score = max(max_score, score)\n",
    "\n",
    "                # Add to active categories if above threshold\n",
    "                if score >= threshold:\n",
    "                    active_categories.append(label)\n",
    "\n",
    "            # Determine if hate speech detected\n",
    "            is_hate_speech = len(active_categories) > 0\n",
    "\n",
    "            # Confidence based on max score\n",
    "            if max_score >= 0.8:\n",
    "                confidence = \"high\"\n",
    "            elif max_score >= 0.6:\n",
    "                confidence = \"medium\"\n",
    "            else:\n",
    "                confidence = \"low\"\n",
    "\n",
    "            return {\n",
    "                \"is_hate_speech\": is_hate_speech,\n",
    "                \"categories\": active_categories,\n",
    "                \"scores\": all_scores,\n",
    "                \"max_score\": max_score,\n",
    "                \"confidence\": confidence,\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting hate speech: {e}\")\n",
    "            import traceback\n",
    "\n",
    "            traceback.print_exc()\n",
    "            return {\n",
    "                \"is_hate_speech\": False,\n",
    "                \"categories\": [],\n",
    "                \"scores\": {},\n",
    "                \"max_score\": 0.0,\n",
    "                \"confidence\": \"error\",\n",
    "            }\n",
    "\n",
    "    # ... rest of the methods remain the same ...\n",
    "    def predict_batch(self, texts, batch_size=16, threshold=0.5):\n",
    "        \"\"\"Predict hate speech for multiple texts efficiently\"\"\"\n",
    "        results = []\n",
    "\n",
    "        # Convert to list if pandas Series\n",
    "        if hasattr(texts, \"tolist\"):\n",
    "            texts = texts.tolist()\n",
    "\n",
    "        print(f\"üîÑ Processing {len(texts)} texts for hate speech analysis...\")\n",
    "        print(f\"üìä Threshold: {threshold} | Batch size: {batch_size}\")\n",
    "\n",
    "        # Process in batches\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Analyzing hate speech\"):\n",
    "            batch = texts[i : i + batch_size]\n",
    "\n",
    "            # Process each text in batch\n",
    "            for text in batch:\n",
    "                result = self.predict_single(text, threshold)\n",
    "                results.append(result)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffde6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test HateSpeechAnalyzer only - FIXED VERSION\n",
    "\n",
    "print(\"üöÄ Testing HateSpeechAnalyzer...\")\n",
    "\n",
    "# Initialize\n",
    "try:\n",
    "    # Buat versi yang lebih simple untuk testing\n",
    "    hate_analyzer = HateSpeechAnalyzer()\n",
    "\n",
    "    # Test with hate speech examples\n",
    "    hate_test_texts = [\n",
    "        (\"Selamat pagi semua!\", \"Expected: Clean\"),\n",
    "        (\"Terima kasih atas bantuannya\", \"Expected: Clean\"),\n",
    "        (\"Dasar bodoh tidak tahu apa-apa\", \"Expected: Abusive/HS_Individual\"),\n",
    "        (\"Agama kalian sesat semua\", \"Expected: HS_Religion/HS_Group\"),\n",
    "        (\"Perempuan memang inferior\", \"Expected: HS_Gender\"),\n",
    "        (\"Orang ras itu memang jelek\", \"Expected: HS_Race\"),\n",
    "        (\"Bunuh saja dia\", \"Expected: HS_Strong\"),\n",
    "        (\"Agak aneh sih orangnya\", \"Expected: HS_Weak\"),\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nüß™ Testing individual hate speech predictions:\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for text, expected in hate_test_texts:\n",
    "        # Hapus debug print untuk testing yang clean\n",
    "        result = hate_analyzer.predict_single(text, threshold=0.5)\n",
    "\n",
    "        print(f\"Text: '{text}'\")\n",
    "        print(f\"   {expected}\")\n",
    "        hate_status = \"üö® YES\" if result[\"is_hate_speech\"] else \"‚úÖ NO\"\n",
    "        print(\n",
    "            f\"   üö® Hate Speech: {hate_status} ({result['max_score']:.3f}) - {result['confidence']}\"\n",
    "        )\n",
    "        if result[\"categories\"]:\n",
    "            print(f\"   üè∑Ô∏è Categories: {', '.join(result['categories'])}\")\n",
    "        print()\n",
    "\n",
    "    print(\"‚úÖ HateSpeechAnalyzer test completed!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in HateSpeechAnalyzer: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb5480f",
   "metadata": {},
   "outputs": [],
   "source": [
    "hate_analyzer = HateSpeechAnalyzer()\n",
    "\n",
    "hate_analyzer.predict_single(\"Dasar lu bego banget sih.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58269e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple debugging version\n",
    "from transformers import pipeline\n",
    "import json\n",
    "\n",
    "print(\"=== DEBUGGING HATE SPEECH MODEL ===\\n\")\n",
    "\n",
    "# Load the model\n",
    "print(\"1. Loading model...\")\n",
    "try:\n",
    "    pipe = pipeline(\n",
    "        \"text-classification\",\n",
    "        model=\"PaceKW/indobert-base-p1-multilabel-indonesian-hate-speech-new\",\n",
    "    )\n",
    "    print(\"‚úÖ Model loaded successfully!\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Test text\n",
    "test_text = \"Kamu jelek banget\"\n",
    "print(f\"2. Testing with: '{test_text}'\\n\")\n",
    "\n",
    "# Method 1: Default prediction (no return_all_scores)\n",
    "print(\"--- Method 1: Default prediction ---\")\n",
    "try:\n",
    "    result1 = pipe(test_text)\n",
    "    print(f\"Type: {type(result1)}\")\n",
    "    print(f\"Content: {result1}\")\n",
    "    print(f\"JSON: {json.dumps(result1, indent=2, ensure_ascii=False)}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\\n\")\n",
    "\n",
    "# Method 2: With return_all_scores=True\n",
    "print(\"--- Method 2: With return_all_scores=True ---\")\n",
    "try:\n",
    "    result2 = pipe(test_text, return_all_scores=True)\n",
    "    print(f\"Type: {type(result2)}\")\n",
    "    print(f\"Length: {len(result2) if hasattr(result2, '__len__') else 'N/A'}\")\n",
    "    print(f\"Content: {result2}\")\n",
    "    print(f\"JSON: {json.dumps(result2, indent=2, ensure_ascii=False)}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\\n\")\n",
    "\n",
    "# Method 3: Multiple texts\n",
    "print(\"--- Method 3: Multiple texts ---\")\n",
    "texts = [\"Kamu jelek\", \"Selamat pagi\"]\n",
    "try:\n",
    "    result3 = pipe(texts)\n",
    "    print(f\"Type: {type(result3)}\")\n",
    "    print(f\"Length: {len(result3) if hasattr(result3, '__len__') else 'N/A'}\")\n",
    "    print(f\"Content: {result3}\")\n",
    "    print(f\"JSON: {json.dumps(result3, indent=2, ensure_ascii=False)}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error: {e}\\n\")\n",
    "\n",
    "# Method 4: Check model config\n",
    "print(\"--- Method 4: Model info ---\")\n",
    "try:\n",
    "    print(f\"Model name: {pipe.model.name_or_path}\")\n",
    "    print(f\"Task: {pipe.task}\")\n",
    "    if hasattr(pipe.model.config, \"id2label\"):\n",
    "        print(f\"Labels: {pipe.model.config.id2label}\")\n",
    "    if hasattr(pipe.model.config, \"problem_type\"):\n",
    "        print(f\"Problem type: {pipe.model.config.problem_type}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error getting model info: {e}\")\n",
    "\n",
    "print(\"\\n=== DEBUG COMPLETE ===\")\n",
    "print(\"Run this first, then tell me what you see!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f69aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 1. Inisialisasi pipeline \"fill-mask\"\n",
    "# Ini akan mengunduh model jika belum ada di cache\n",
    "print(\"Mengunduh model (jika diperlukan)...\")\n",
    "tebak_kata = pipeline(\"fill-mask\", model=\"cahya/bert-base-indonesian-1.5G\")\n",
    "print(\"Model siap digunakan.\")\n",
    "\n",
    "# 2. Siapkan beberapa kalimat tes\n",
    "kalimat1 = \"Ibu kota negara Indonesia adalah [MASK].\"\n",
    "kalimat2 = \"Orang yang bekerja di rumah sakit biasanya adalah seorang [MASK].\"\n",
    "kalimat3 = \"Setelah lelah bekerja seharian, enaknya minum [MASK] dingin.\"\n",
    "kalimat4 = \"Dia membeli mobil baru berwarna [MASK].\"\n",
    "\n",
    "# 3. Lakukan prediksi dan lihat hasilnya\n",
    "print(f\"\\n--- Tes untuk: '{kalimat1}' ---\")\n",
    "hasil1 = tebak_kata(kalimat1)\n",
    "for prediksi in hasil1:\n",
    "    print(\n",
    "        f\"Kata: {prediksi['token_str']:<15} | Skor Keyakinan: {prediksi['score']:.4f}\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n--- Tes untuk: '{kalimat2}' ---\")\n",
    "hasil2 = tebak_kata(kalimat2, top_k=3)  # Minta 3 tebakan teratas\n",
    "for prediksi in hasil2:\n",
    "    print(f\"Kalimat Lengkap: {prediksi['sequence']}\")\n",
    "\n",
    "print(f\"\\n--- Tes untuk: '{kalimat3}' ---\")\n",
    "hasil3 = tebak_kata(kalimat3, top_k=3)\n",
    "for prediksi in hasil3:\n",
    "    print(f\"Kalimat Lengkap: {prediksi['sequence']}\")\n",
    "\n",
    "print(f\"\\n--- Tes untuk: '{kalimat4}' ---\")\n",
    "hasil4 = tebak_kata(kalimat4, top_k=3)\n",
    "for prediksi in hasil4:\n",
    "    print(f\"Kalimat Lengkap: {prediksi['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c04c4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
